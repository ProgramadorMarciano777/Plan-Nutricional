{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\informatica1\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import anthropic\n",
    "import google.generativeai as genai\n",
    "# import openai\n",
    "from llamaapi import LlamaAPI\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8edc3ba8c99c7df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input and Output path\n",
    "file_path = 'input.txt'\n",
    "input_menus_dir = 'input/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c0e05fef8e6966e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your OpenAI API key\n",
    "OPENAI_API_KEY = \"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\"\n",
    "GEMINI_API_KEY = \"AIzaSyAwlokz3qh0_dtFs7KKw1wqg47BuODpn5I\"\n",
    "LLAMA_API_KEY = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1cafebf6fc8d915c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your models\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "# openai.api_key = OPENAI_API_KEY\n",
    "openai_client = OpenAI(api_key = OPENAI_API_KEY)\n",
    "llama_client = LlamaAPI(LLAMA_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9f323956002404d",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define the template prompt as a global parameter\n",
    "TEMPLATE_PROMPT = \"\"\"\n",
    "User preferences are: {user_preferences}.\n",
    "User restrictions are: {user_restrictions}.\n",
    "User's goal is to create a meal plan that meets the following targets:\n",
    "- Total daily calories: {total_calories} kcal.\n",
    "- Total daily protein: {target_protein}g.\n",
    "- Total daily sugar: {target_sugar}g.\n",
    "The plan must include:\n",
    "- Breakfast, lunch, dinner, and snacks.\n",
    "- The calorie count for each meal (e.g., \"Breakfast: 400 kcal\").\n",
    "- At the end of each meal plan option, provide the total calories, total fat, total protein, and total carbohydrate.\n",
    "- For each item in the meal plans (e.g., breakfast, lunch), specify the exact portion sizes, including the number of items or volume (e.g., \"1 Kit Kat bar (45g),\" \"1 hamburger with a 150g beef patty, bun, and lettuce\").\n",
    "- Provide a short recipe for each item in the meal plan, detailing how it can be prepared (e.g., \"Grill the patty for 5 minutes, then assemble with lettuce, tomato, and a bun\").\n",
    "Provide three different meal plan options for diversity.\n",
    "Use familiar dishes instead of listing individual food items. For example, use \"hamburger\" instead of \"150 grams of meat with bun and lettuce.\"\n",
    "Ensure the plan adheres to the user's preferences and restrictions and meets the specified targets while maintaining a balanced nutritional profile.\n",
    "Here is the available items for generating the meal plan:\n",
    "{menu_input}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "386dbcbaf5a7fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_menus(input_dir, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    menus = []\n",
    "    with open(input_dir, 'r') as file:\n",
    "        content = file.read()\n",
    "        # Split menus by separators (---)\n",
    "        menu_sections = content.strip().split('---')\n",
    "        \n",
    "        for idx, section in enumerate(menu_sections):\n",
    "            lines = section.strip().split('\\n')\n",
    "            menu = {\"foods\": {}, \"energy\": None, \"protein\": None, \"sugar\": None}\n",
    "            \n",
    "            # Process each line in the menu section\n",
    "            for line in lines:\n",
    "                if line.startswith(\"Food_\"):\n",
    "                    # Parse food items and their quantities\n",
    "                    food, quantity = line.split('=')\n",
    "                    menu[\"foods\"][food.strip()] = float(quantity.strip())\n",
    "                elif line.startswith(\"energy\"):\n",
    "                    # Parse energy value\n",
    "                    menu[\"energy\"] = float(line.split()[1])\n",
    "                elif line.startswith(\"protein\"):\n",
    "                    # Parse protein value\n",
    "                    menu[\"protein\"] = float(line.split()[1])\n",
    "                elif line.startswith(\"sugar\"):\n",
    "                    # Parse sugar value\n",
    "                    menu[\"sugar\"] = float(line.split()[1])\n",
    "            \n",
    "            # Add the menu to the list\n",
    "            menus.append(menu)\n",
    "            \n",
    "            # Save the menu to a separate text file\n",
    "            menu_file_path = os.path.join(output_dir, f\"menu_{idx + 1}.txt\")\n",
    "            with open(menu_file_path, \"w\", encoding=\"utf-8\") as menu_file:\n",
    "                # Write the foods\n",
    "                for food, quantity in menu[\"foods\"].items():\n",
    "                    menu_file.write(f\"{food} = {quantity}\\n\")\n",
    "                # Write the nutritional values\n",
    "                menu_file.write(\"--------------------------\\n\")\n",
    "                menu_file.write(f\"energy = {menu['energy']}\\n\")\n",
    "                menu_file.write(f\"protein = {menu['protein']}\\n\")\n",
    "                menu_file.write(f\"sugar = {menu['sugar']}\\n\")\n",
    "    \n",
    "    return menus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b59cdf04dc65c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompts_from_menus(menus, user_preferences=\"\", user_restrictions=\"\"):\n",
    "    prompts = []\n",
    "    \n",
    "    for menu in menus:\n",
    "        # Format the menu items into a string\n",
    "        menu_input = \"\\n\".join([f\"{food} = {quantity}\" for food, quantity in menu['foods'].items()])\n",
    "        \n",
    "        # Fill the template with data from the menu\n",
    "        prompt = TEMPLATE_PROMPT.format(\n",
    "            user_preferences=user_preferences,\n",
    "            user_restrictions=user_restrictions,\n",
    "            total_calories=menu['energy'],\n",
    "            target_protein=menu['protein'],\n",
    "            target_sugar=menu['sugar'],\n",
    "            menu_input=menu_input\n",
    "        )\n",
    "        \n",
    "        prompts.append(prompt.strip())\n",
    "    \n",
    "    return prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26b60798c4babf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_llm_api(prompts, company, model_name, max_tokens=2000, temperature=0.7):\n",
    "    responses = []\n",
    "    \n",
    "    for i, prompt in enumerate(prompts):\n",
    "        print(f\"Processing prompt {i + 1} of {len(prompts)}...\")\n",
    "        \n",
    "        try:\n",
    "            if company == \"openai\":\n",
    "                response = openai_client.chat.completions.create(\n",
    "                    model=model_name,\n",
    "                    messages=[\n",
    "                        {\"role\": \"user\", \"content\": prompt},\n",
    "                    ],\n",
    "                    max_tokens=max_tokens,\n",
    "                    temperature=temperature,\n",
    "                )\n",
    "                meal_plan = response.choices[0].message.content\n",
    "                responses.append(meal_plan)\n",
    "                \n",
    "            elif company == \"google\":\n",
    "                model = genai.GenerativeModel(model_name)\n",
    "                response = model.generate_content(prompt)\n",
    "                responses.append(response)\n",
    "\n",
    "            elif company == \"meta\":\n",
    "                api_request_json = {\"model\": model_name, \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "                                    \"max_tokens\": max_tokens, \"temperature\": temperature, }\n",
    "                response = llama_client.run(api_request_json)\n",
    "                content = response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "                responses.append(content)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing prompt {i + 1}: {e}\")\n",
    "            responses.append(f\"Error: {e}\")\n",
    "    \n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ac4efa5574d8064",
   "metadata": {},
   "outputs": [],
   "source": [
    "menus = process_menus(input_dir=file_path, output_dir=input_menus_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64e931ce9b0084ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = generate_prompts_from_menus(menus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4f8ba060ababb3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing prompt 1 of 30...\n",
      "Processing prompt 2 of 30...\n",
      "Processing prompt 3 of 30...\n",
      "Processing prompt 4 of 30...\n",
      "Processing prompt 5 of 30...\n",
      "Processing prompt 6 of 30...\n",
      "Processing prompt 7 of 30...\n",
      "Processing prompt 8 of 30...\n",
      "Processing prompt 9 of 30...\n",
      "Processing prompt 10 of 30...\n",
      "Processing prompt 11 of 30...\n",
      "Processing prompt 12 of 30...\n",
      "Processing prompt 13 of 30...\n",
      "Processing prompt 14 of 30...\n",
      "Processing prompt 15 of 30...\n",
      "Processing prompt 16 of 30...\n",
      "Processing prompt 17 of 30...\n",
      "Processing prompt 18 of 30...\n",
      "Processing prompt 19 of 30...\n",
      "Processing prompt 20 of 30...\n",
      "Processing prompt 21 of 30...\n",
      "Processing prompt 22 of 30...\n",
      "Processing prompt 23 of 30...\n",
      "Processing prompt 24 of 30...\n",
      "Processing prompt 25 of 30...\n",
      "Processing prompt 26 of 30...\n",
      "Processing prompt 27 of 30...\n",
      "Processing prompt 28 of 30...\n",
      "Processing prompt 29 of 30...\n",
      "Processing prompt 30 of 30...\n"
     ]
    }
   ],
   "source": [
    "model_name=\"gpt-4o\"\n",
    "company=\"openai\"\n",
    "responses = call_llm_api(prompts=prompts, company=company, model_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "28c762992dfccd49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved response 1 to output_gpt-4o/response_1.txt\n",
      "Saved response 2 to output_gpt-4o/response_2.txt\n",
      "Saved response 3 to output_gpt-4o/response_3.txt\n",
      "Saved response 4 to output_gpt-4o/response_4.txt\n",
      "Saved response 5 to output_gpt-4o/response_5.txt\n",
      "Saved response 6 to output_gpt-4o/response_6.txt\n",
      "Saved response 7 to output_gpt-4o/response_7.txt\n",
      "Saved response 8 to output_gpt-4o/response_8.txt\n",
      "Saved response 9 to output_gpt-4o/response_9.txt\n",
      "Saved response 10 to output_gpt-4o/response_10.txt\n",
      "Saved response 11 to output_gpt-4o/response_11.txt\n",
      "Saved response 12 to output_gpt-4o/response_12.txt\n",
      "Saved response 13 to output_gpt-4o/response_13.txt\n",
      "Saved response 14 to output_gpt-4o/response_14.txt\n",
      "Saved response 15 to output_gpt-4o/response_15.txt\n",
      "Saved response 16 to output_gpt-4o/response_16.txt\n",
      "Saved response 17 to output_gpt-4o/response_17.txt\n",
      "Saved response 18 to output_gpt-4o/response_18.txt\n",
      "Saved response 19 to output_gpt-4o/response_19.txt\n",
      "Saved response 20 to output_gpt-4o/response_20.txt\n",
      "Saved response 21 to output_gpt-4o/response_21.txt\n",
      "Saved response 22 to output_gpt-4o/response_22.txt\n",
      "Saved response 23 to output_gpt-4o/response_23.txt\n",
      "Saved response 24 to output_gpt-4o/response_24.txt\n",
      "Saved response 25 to output_gpt-4o/response_25.txt\n",
      "Saved response 26 to output_gpt-4o/response_26.txt\n",
      "Saved response 27 to output_gpt-4o/response_27.txt\n",
      "Saved response 28 to output_gpt-4o/response_28.txt\n",
      "Saved response 29 to output_gpt-4o/response_29.txt\n",
      "Saved response 30 to output_gpt-4o/response_30.txt\n"
     ]
    }
   ],
   "source": [
    "# Save each response to a file\n",
    "out_dir = f\"output_{model_name}\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "for i, response in enumerate(responses):\n",
    "    with open(out_dir + f\"/response_{i + 1}.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(response)\n",
    "    print(f\"Saved response {i + 1} to \" + out_dir + f\"/response_{i + 1}.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
